#!/usr/bin/env bash

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
ROOT_DIR="$(cd "${SCRIPT_DIR}/.." && pwd)"
STATE_DIR="${ROOT_DIR}/state"
TX_DIR="${STATE_DIR}/transactions"
LOG_DIR="${STATE_DIR}/logs"
CONFLICT_DIR="${STATE_DIR}/conflicts"
WORK_DIR="${STATE_DIR}/work"
CACHE_WHEELS_DIR="${ROOT_DIR}/cache/wheels"
OPS_DIR="${STATE_DIR}/ops"
CONFIG_FILE="${ROOT_DIR}/config.toml"
PROJECT_FILE="${ROOT_DIR}/pyproject.toml"
PROJECT_TEMPLATE_FILE="${ROOT_DIR}/pyproject.toml.template"
LOCK_FILE="${ROOT_DIR}/uv.lock"
PLUGINS_FILE="${STATE_DIR}/plugins.json"
PID_FILE="${STATE_DIR}/comfyui.pid"

TX_FILE=""

detect_python_bin() {
    if [ -n "${PYTHON_CMD:-}" ] && command -v "${PYTHON_CMD}" >/dev/null 2>&1; then
        echo "${PYTHON_CMD}"
        return 0
    fi
    if command -v python3 >/dev/null 2>&1; then
        echo "python3"
        return 0
    fi
    if command -v python >/dev/null 2>&1; then
        echo "python"
        return 0
    fi
    return 1
}

PYTHON_BIN="$(detect_python_bin || true)"

timestamp_utc() {
    date -u +"%Y-%m-%dT%H:%M:%SZ"
}

require_python() {
    if [ -z "${PYTHON_BIN}" ]; then
        echo "ERROR: python3/python is required" >&2
        exit 1
    fi
}

require_cmd() {
    local cmd="$1"
    if ! command -v "$cmd" >/dev/null 2>&1; then
        echo "ERROR: command not found: $cmd" >&2
        exit 1
    fi
}

config_get() {
    local key="$1"
    local default_value="$2"
    require_python
    "${PYTHON_BIN}" - "$CONFIG_FILE" "$key" "$default_value" <<'PY'
import pathlib
import sys

try:
    import tomllib
except ModuleNotFoundError:
    try:
        import tomli as tomllib
    except ModuleNotFoundError:
        print(sys.argv[3])
        raise SystemExit(0)

cfg_path = pathlib.Path(sys.argv[1])
key = sys.argv[2]
default = sys.argv[3]

if not cfg_path.exists():
    print(default)
    raise SystemExit(0)

try:
    data = tomllib.loads(cfg_path.read_text(encoding="utf-8"))
except Exception:
    print(default)
    raise SystemExit(0)

cur = data
for part in key.split('.'):
    if isinstance(cur, dict) and part in cur:
        cur = cur[part]
    else:
        print(default)
        raise SystemExit(0)

if isinstance(cur, bool):
    print("true" if cur else "false")
elif isinstance(cur, (int, float)):
    print(cur)
elif isinstance(cur, list):
    print(",".join(str(x) for x in cur))
elif cur is None:
    print(default)
else:
    print(str(cur))
PY
}

ensure_layout() {
    mkdir -p "${STATE_DIR}" "${TX_DIR}" "${LOG_DIR}" "${CONFLICT_DIR}" "${WORK_DIR}" "${CACHE_WHEELS_DIR}" "${OPS_DIR}" "${ROOT_DIR}/.venv-candidate"
    if [ ! -f "${PLUGINS_FILE}" ]; then
        printf '[]\n' > "${PLUGINS_FILE}"
    fi
    migrate_plugins_registry
}

ensure_project_file_for_init() {
    if [ -f "$PROJECT_FILE" ]; then
        return 0
    fi
    if [ -f "$PROJECT_TEMPLATE_FILE" ]; then
        cp "$PROJECT_TEMPLATE_FILE" "$PROJECT_FILE"
        echo "Created local pyproject.toml from template."
        echo "  template: $PROJECT_TEMPLATE_FILE"
        return 0
    fi
    echo "ERROR: missing $PROJECT_FILE and template $PROJECT_TEMPLATE_FILE" >&2
    echo "Hint: restore pyproject.toml.template from repository, then rerun init." >&2
    exit 1
}

prod_env_path() {
    local rel
    rel="$(config_get "runtime.prod_env" ".venv-prod")"
    echo "${ROOT_DIR}/${rel}"
}

candidate_root_path() {
    local rel
    rel="$(config_get "runtime.candidate_root" ".venv-candidate")"
    echo "${ROOT_DIR}/${rel}"
}

comfyui_dir_path() {
    config_get "paths.comfyui_dir" "/opt/comfyui"
}

tx_timeout_seconds() {
    config_get "tx.timeout_seconds" "120"
}

core_packages_csv() {
    config_get "policy.core_packages" "torch,torchvision,torchaudio,xformers,triton,onnxruntime,onnxruntime-gpu,numpy"
}

ops_retention_count() {
    local value
    value="$(config_get "ops.retention_count" "")"
    if [ -n "$value" ]; then
        echo "$value"
        return 0
    fi
    value="$(config_get "rollback.snapshot_retention" "")"
    if [ -n "$value" ]; then
        echo "$value"
        return 0
    fi
    echo "100"
}

smoke_test_cmd() {
    config_get "tx.smoke_test_cmd" ""
}

normalize_group_name() {
    local raw="${1:-}"
    local out
    out="$(echo "$raw" | tr '[:upper:]' '[:lower:]' | sed -E 's/[-_.]+/-/g; s/^-+//; s/-+$//')"
    echo "$out"
}

group_name_for_node() {
    local node_id="$1"
    local norm
    norm="$(normalize_group_name "$node_id")"
    if [ -z "$norm" ]; then
        norm="$node_id"
    fi
    echo "node-${norm}"
}

new_txid() {
    require_python
    "${PYTHON_BIN}" - <<'PY'
import datetime
import uuid
print(datetime.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ") + "-" + uuid.uuid4().hex[:8])
PY
}

new_op_id() {
    require_python
    "${PYTHON_BIN}" - <<'PY'
import datetime
import uuid
print(datetime.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ") + "-op-" + uuid.uuid4().hex[:8])
PY
}

file_sha256() {
    local file_path="$1"
    require_python
    "${PYTHON_BIN}" - "$file_path" <<'PY'
import hashlib
import pathlib
import sys

p = pathlib.Path(sys.argv[1])
if not p.exists():
    print("")
    raise SystemExit(0)
print(hashlib.sha256(p.read_bytes()).hexdigest())
PY
}

ops_prune() {
    ensure_layout
    local retention
    retention="$(ops_retention_count)"
    require_python
    "${PYTHON_BIN}" - "$OPS_DIR" "$retention" <<'PY'
import pathlib
import shutil
import sys

root = pathlib.Path(sys.argv[1])
try:
    retention = int(sys.argv[2])
except Exception:
    retention = 100
if retention < 1:
    retention = 1

ops = sorted([p for p in root.iterdir() if p.is_dir()], key=lambda p: p.name)
if len(ops) <= retention:
    raise SystemExit(0)

for p in ops[:-retention]:
    shutil.rmtree(p, ignore_errors=True)
PY
}

op_begin() {
    local kind="$1"
    local ref="$2"
    ensure_layout
    require_python

    local op_id op_dir backup_dir
    op_id="$(new_op_id)"
    op_dir="${OPS_DIR}/${op_id}"
    backup_dir="${op_dir}/backup"
    mkdir -p "$backup_dir"

    cp "$PROJECT_FILE" "${backup_dir}/pyproject.toml"
    if [ -f "$LOCK_FILE" ]; then
        cp "$LOCK_FILE" "${backup_dir}/uv.lock"
    else
        : > "${backup_dir}/uv.lock"
    fi
    cp "$PLUGINS_FILE" "${backup_dir}/plugins.json"

    "${PYTHON_BIN}" - "$op_dir/meta.json" "$op_id" "$kind" "$ref" "$(timestamp_utc)" "$backup_dir" \
        "$(file_sha256 "$PROJECT_FILE")" "$(file_sha256 "$LOCK_FILE")" "$(file_sha256 "$PLUGINS_FILE")" <<'PY'
import json
import pathlib
import sys

meta_path = pathlib.Path(sys.argv[1])
meta = {
    "op_id": sys.argv[2],
    "kind": sys.argv[3],
    "ref": sys.argv[4],
    "status": "running",
    "started_at": sys.argv[5],
    "ended_at": "",
    "files": {
        "pyproject.toml": {"pre_sha256": sys.argv[7], "post_sha256": ""},
        "uv.lock": {"pre_sha256": sys.argv[8], "post_sha256": ""},
        "plugins.json": {"pre_sha256": sys.argv[9], "post_sha256": ""},
    },
    "backup_dir": sys.argv[6],
    "undoable": False,
    "note": "",
}
meta_path.write_text(json.dumps(meta, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
PY

    echo "$op_id"
}

op_restore_backup() {
    local op_id="$1"
    local op_dir="${OPS_DIR}/${op_id}"
    local backup_dir="${op_dir}/backup"
    if [ ! -d "$backup_dir" ]; then
        echo "ERROR: op backup missing: $op_id" >&2
        return 1
    fi
    cp "${backup_dir}/pyproject.toml" "$PROJECT_FILE"
    cp "${backup_dir}/uv.lock" "$LOCK_FILE"
    cp "${backup_dir}/plugins.json" "$PLUGINS_FILE"
}

op_finalize() {
    local op_id="$1"
    local status="$2"
    local undoable="$3"
    local note="$4"
    local op_dir="${OPS_DIR}/${op_id}"
    require_python
    "${PYTHON_BIN}" - "$op_dir/meta.json" "$status" "$undoable" "$note" "$(timestamp_utc)" \
        "$(file_sha256 "$PROJECT_FILE")" "$(file_sha256 "$LOCK_FILE")" "$(file_sha256 "$PLUGINS_FILE")" <<'PY'
import json
import pathlib
import sys

meta_path = pathlib.Path(sys.argv[1])
status = sys.argv[2]
undoable = sys.argv[3].lower() == "true"
note = sys.argv[4]
ended_at = sys.argv[5]
post_pyproject = sys.argv[6]
post_lock = sys.argv[7]
post_plugins = sys.argv[8]

data = json.loads(meta_path.read_text(encoding="utf-8"))
data["status"] = status
data["ended_at"] = ended_at
data["undoable"] = undoable
data["note"] = note
files = data.get("files", {})
files.setdefault("pyproject.toml", {})["post_sha256"] = post_pyproject
files.setdefault("uv.lock", {})["post_sha256"] = post_lock
files.setdefault("plugins.json", {})["post_sha256"] = post_plugins
data["files"] = files
meta_path.write_text(json.dumps(data, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
PY
}

plugin_install_abs_path() {
    local rel_path="$1"
    local comfyui_dir
    comfyui_dir="$(comfyui_dir_path)"
    echo "${comfyui_dir}/${rel_path}"
}

write_group_deps() {
    local pyproject_file="$1"
    local group="$2"
    local out_file="$3"
    require_python
    "${PYTHON_BIN}" - "$pyproject_file" "$group" "$out_file" <<'PY'
import pathlib
import sys

try:
    import tomllib
except ModuleNotFoundError:
    import tomli as tomllib

pyproject = pathlib.Path(sys.argv[1])
group = sys.argv[2]
out_file = pathlib.Path(sys.argv[3])

deps = []
if pyproject.exists():
    try:
        data = tomllib.loads(pyproject.read_text(encoding="utf-8"))
    except Exception:
        data = {}
    groups = data.get("dependency-groups", {})
    raw = groups.get(group, []) if isinstance(groups, dict) else []
    if isinstance(raw, list):
        deps = [str(x).strip() for x in raw if str(x).strip()]

out_file.write_text("\n".join(deps) + ("\n" if deps else ""), encoding="utf-8")
PY
}

migrate_plugins_registry() {
    require_python
    "${PYTHON_BIN}" - "$PLUGINS_FILE" "$PROJECT_FILE" "$(comfyui_dir_path)" "$(timestamp_utc)" <<'PY'
import json
import pathlib
import sys

try:
    import tomllib
except ModuleNotFoundError:
    import tomli as tomllib

plugins_file = pathlib.Path(sys.argv[1])
project_file = pathlib.Path(sys.argv[2])
comfyui_dir = pathlib.Path(sys.argv[3]).resolve()
ts_now = sys.argv[4]

def load_groups():
    if not project_file.exists():
        return {}
    try:
        data = tomllib.loads(project_file.read_text(encoding="utf-8"))
    except Exception:
        return {}
    groups = data.get("dependency-groups", {})
    if not isinstance(groups, dict):
        return {}
    out = {}
    for k, v in groups.items():
        if isinstance(v, list):
            out[str(k)] = [str(x).strip() for x in v if str(x).strip()]
    return out

def norm_token(value: str):
    import re
    return re.sub(r"[-_.]+", "-", str(value).strip().lower()).strip("-")

def to_relpath(value: str, node_id: str):
    if value:
        p = pathlib.Path(value)
        if not p.is_absolute():
            return str(p).replace("\\", "/")
        try:
            return str(p.resolve().relative_to(comfyui_dir)).replace("\\", "/")
        except Exception:
            raise SystemExit(f"ERROR: plugins.json path cannot be converted to install_relpath for node '{node_id}': {value}")
    return f"custom_nodes/{node_id}"

groups = load_groups()
try:
    raw = json.loads(plugins_file.read_text(encoding="utf-8")) if plugins_file.exists() else []
except Exception:
    raw = []
if not isinstance(raw, list):
    raw = []

new_data = []
for item in raw:
    if not isinstance(item, dict):
        continue
    node_id = str(item.get("id", "")).strip()
    if not node_id:
        continue
    group = norm_token(str(item.get("group") or f"node-{node_id}"))
    if not group:
        group = f"node-{norm_token(node_id)}"
    relpath = str(item.get("install_relpath") or "").strip()
    if not relpath:
        relpath = to_relpath(str(item.get("path", "")).strip(), node_id)
    managed = item.get("managed_deps", [])
    if not isinstance(managed, list):
        managed = []
    managed = [str(x).strip() for x in managed if str(x).strip()]
    if not managed:
        managed = groups.get(group, [])
    created_at = str(item.get("created_at") or ts_now)
    updated_at = str(item.get("updated_at") or created_at)
    new_data.append({
        "id": node_id,
        "git_url": str(item.get("git_url", "")),
        "ref": str(item.get("ref", "")),
        "install_relpath": relpath,
        "group": group,
        "managed_deps": managed,
        "enabled": bool(item.get("enabled", True)),
        "created_at": created_at,
        "updated_at": updated_at,
    })

plugins_file.write_text(json.dumps(new_data, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
PY
}

collect_freeze_file() {
    local env_path="$1"
    local out_file="$2"
    if [ -d "$env_path" ]; then
        UV_PROJECT_ENVIRONMENT="$env_path" uv pip freeze 2>/dev/null | sort > "$out_file" || true
    else
        : > "$out_file"
    fi
}

load_tx() {
    local txid="$1"
    TX_FILE="${TX_DIR}/${txid}.json"
    if [ ! -f "$TX_FILE" ]; then
        echo "ERROR: transaction not found: $txid" >&2
        exit 2
    fi
}

save_tx() {
    local tx_file="$1"
    local tmp_file="$2"
    cp "$tmp_file" "$tx_file"
}
ensure_plugin_exists() {
    local node_id="$1"
    require_python
    "${PYTHON_BIN}" - "$PLUGINS_FILE" "$node_id" <<'PY'
import json
import pathlib
import sys

path = pathlib.Path(sys.argv[1])
node_id = sys.argv[2]

if not path.exists():
    raise SystemExit(1)

try:
    data = json.loads(path.read_text(encoding="utf-8"))
except Exception:
    raise SystemExit(2)

ok = any(isinstance(x, dict) and x.get("id") == node_id for x in data)
raise SystemExit(0 if ok else 3)
PY
}

plugin_get_meta() {
    local node_id="$1"
    local out_json="$2"
    require_python
    "${PYTHON_BIN}" - "$PLUGINS_FILE" "$node_id" "$out_json" <<'PY'
import json
import pathlib
import sys

path = pathlib.Path(sys.argv[1])
node_id = sys.argv[2]
out_json = pathlib.Path(sys.argv[3])

if not path.exists():
    raise SystemExit(2)

try:
    data = json.loads(path.read_text(encoding="utf-8"))
except Exception:
    raise SystemExit(3)

for item in data:
    if isinstance(item, dict) and item.get("id") == node_id:
        out_json.write_text(json.dumps(item, ensure_ascii=True), encoding="utf-8")
        raise SystemExit(0)

raise SystemExit(1)
PY
}

plugin_remove_record() {
    local node_id="$1"
    require_python
    "${PYTHON_BIN}" - "$PLUGINS_FILE" "$node_id" <<'PY'
import json
import pathlib
import sys

path = pathlib.Path(sys.argv[1])
node_id = sys.argv[2]

data = []
if path.exists():
    try:
        data = json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        data = []

if not isinstance(data, list):
    data = []

new_data = [x for x in data if not (isinstance(x, dict) and x.get("id") == node_id)]
path.write_text(json.dumps(new_data, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
PY
}

plugin_update_after_promote() {
    local node_id="$1"
    local project_file="$2"
    require_python
    "${PYTHON_BIN}" - "$PLUGINS_FILE" "$project_file" "$node_id" "$(timestamp_utc)" <<'PY'
import json
import pathlib
import re
import sys

try:
    import tomllib
except ModuleNotFoundError:
    import tomli as tomllib

plugins_path = pathlib.Path(sys.argv[1])
project_file = pathlib.Path(sys.argv[2])
node_id = sys.argv[3]
ts_now = sys.argv[4]
try:
    data = json.loads(plugins_path.read_text(encoding="utf-8")) if plugins_path.exists() else []
except Exception:
    data = []

if not isinstance(data, list):
    data = []

groups = {}
if project_file.exists():
    try:
        doc = tomllib.loads(project_file.read_text(encoding="utf-8"))
    except Exception:
        doc = {}
    raw_groups = doc.get("dependency-groups", {})
    if isinstance(raw_groups, dict):
        for k, v in raw_groups.items():
            if isinstance(v, list):
                groups[str(k)] = [str(x).strip() for x in v if str(x).strip()]

found = False
for item in data:
    if isinstance(item, dict) and item.get("id") == node_id:
        group = re.sub(r"[-_.]+", "-", str(item.get("group") or f"node-{node_id}").lower()).strip("-")
        if not group:
            group = f"node-{re.sub(r'[-_.]+', '-', node_id.lower()).strip('-')}"
        item["group"] = group
        item["managed_deps"] = groups.get(group, [])
        item["updated_at"] = ts_now
        found = True
        break

if not found:
    group = f"node-{re.sub(r'[-_.]+', '-', node_id.lower()).strip('-')}"
    data.append({
        "id": node_id,
        "git_url": "",
        "ref": "",
        "install_relpath": f"custom_nodes/{node_id}",
        "group": group,
        "managed_deps": groups.get(group, []),
        "enabled": True,
        "created_at": ts_now,
        "updated_at": ts_now,
    })

plugins_path.write_text(json.dumps(data, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
PY
}

build_workdir_for_tx() {
    local txid="$1"
    local workdir="${WORK_DIR}/${txid}"
    rm -rf "$workdir"
    mkdir -p "$workdir"
    cp "$PROJECT_FILE" "$workdir/pyproject.toml"
    if [ -f "$LOCK_FILE" ]; then
        cp "$LOCK_FILE" "$workdir/uv.lock"
    fi
    echo "$workdir"
}

apply_plan_in_workdir() {
    local workdir="$1"
    local group_name="$2"
    local direct_file="$3"
    local override_file="$4"
    local log_file="$5"

    (
        cd "$workdir"
        if [ -s "$direct_file" ]; then
            while IFS= read -r spec; do
                [ -z "$spec" ] && continue
                uv add --group "$group_name" "$spec"
            done < "$direct_file"
        fi

        if [ -s "$override_file" ]; then
            while IFS= read -r spec; do
                [ -z "$spec" ] && continue
                uv add --group overrides "$spec"
            done < "$override_file"
        fi

        uv lock
    ) >"$log_file" 2>&1
}

tx_get_field() {
    local tx_file="$1"
    local field_path="$2"
    local default_value="${3:-}"
    require_python
    "${PYTHON_BIN}" - "$tx_file" "$field_path" "$default_value" <<'PY'
import json
import pathlib
import sys

path = pathlib.Path(sys.argv[1])
field_path = sys.argv[2]
default = sys.argv[3]

if not path.exists():
    print(default)
    raise SystemExit(0)

data = json.loads(path.read_text(encoding="utf-8"))
cur = data
for part in field_path.split('.'):
    if isinstance(cur, dict) and part in cur:
        cur = cur[part]
    else:
        print(default)
        raise SystemExit(0)

if isinstance(cur, list):
    print("\n".join(str(x) for x in cur))
elif isinstance(cur, dict):
    print(json.dumps(cur, ensure_ascii=True))
else:
    print(cur)
PY
}
tx_update_status() {
    local tx_file="$1"
    local status="$2"
    require_python
    "${PYTHON_BIN}" - "$tx_file" "$status" "$(timestamp_utc)" <<'PY'
import json
import pathlib
import sys

path = pathlib.Path(sys.argv[1])
status = sys.argv[2]
ended_at = sys.argv[3]

data = json.loads(path.read_text(encoding="utf-8"))
data["status"] = status
data["ended_at"] = ended_at
path.write_text(json.dumps(data, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
PY
}

tx_set_plan() {
    local tx_file="$1"
    local plan_json="$2"
    require_python
    "${PYTHON_BIN}" - "$tx_file" "$plan_json" <<'PY'
import json
import pathlib
import sys

tx_file = pathlib.Path(sys.argv[1])
plan_file = pathlib.Path(sys.argv[2])

tx = json.loads(tx_file.read_text(encoding="utf-8"))
plan = json.loads(plan_file.read_text(encoding="utf-8"))

tx["promotion_plan"] = plan
tx_file.write_text(json.dumps(tx, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
PY
}

tx_set_resolution_pins() {
    local tx_file="$1"
    local pins_file="$2"
    local status="$3"
    require_python
    "${PYTHON_BIN}" - "$tx_file" "$pins_file" "$status" "$(timestamp_utc)" <<'PY'
import json
import pathlib
import sys

tx_file = pathlib.Path(sys.argv[1])
pins_file = pathlib.Path(sys.argv[2])
status = sys.argv[3]
ended_at = sys.argv[4]

tx = json.loads(tx_file.read_text(encoding="utf-8"))
pins = []
if pins_file.exists():
    pins = [line.strip() for line in pins_file.read_text(encoding="utf-8").splitlines() if line.strip()]

seen = set()
merged = []
for item in tx.get("resolution_pins", []) + pins:
    if item not in seen:
        seen.add(item)
        merged.append(item)

tx["resolution_pins"] = merged
tx["status"] = status
tx["ended_at"] = ended_at
tx_file.write_text(json.dumps(tx, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
PY
}

tx_set_conflict() {
    local tx_file="$1"
    local conflict_path="$2"
    local status="$3"
    local pre_op_id="$4"
    require_python
    "${PYTHON_BIN}" - "$tx_file" "$conflict_path" "$status" "$pre_op_id" "$(timestamp_utc)" <<'PY'
import json
import pathlib
import sys

tx_file = pathlib.Path(sys.argv[1])
conflict_path = sys.argv[2]
status = sys.argv[3]
pre_op_id = sys.argv[4]
ended_at = sys.argv[5]

tx = json.loads(tx_file.read_text(encoding="utf-8"))
tx["status"] = status
tx["ended_at"] = ended_at
tx["conflict_report"] = conflict_path
prom = tx.get("promotion", {})
prom["status"] = "lock_failed"
prom["pre_op_id"] = pre_op_id
tx["promotion"] = prom
tx_file.write_text(json.dumps(tx, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
PY
}

tx_set_promoted() {
    local tx_file="$1"
    local op_id="$2"
    local reason="$3"
    require_python
    "${PYTHON_BIN}" - "$tx_file" "$op_id" "$reason" "$(timestamp_utc)" <<'PY'
import json
import pathlib
import sys

tx_file = pathlib.Path(sys.argv[1])
op_id = sys.argv[2]
reason = sys.argv[3]
ended_at = sys.argv[4]

tx = json.loads(tx_file.read_text(encoding="utf-8"))
tx["status"] = "promoted"
tx["ended_at"] = ended_at
prom = tx.get("promotion", {})
prom["status"] = "promoted"
prom["op_id"] = op_id
prom["reason"] = reason
tx["promotion"] = prom
tx_file.write_text(json.dumps(tx, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
PY
}

tx_set_promote_failed() {
    local tx_file="$1"
    local pre_op_id="$2"
    local error_msg="$3"
    require_python
    "${PYTHON_BIN}" - "$tx_file" "$pre_op_id" "$error_msg" "$(timestamp_utc)" <<'PY'
import json
import pathlib
import sys

tx_file = pathlib.Path(sys.argv[1])
pre_op_id = sys.argv[2]
error_msg = sys.argv[3]
ended_at = sys.argv[4]

tx = json.loads(tx_file.read_text(encoding="utf-8"))
tx["status"] = "promote_failed"
tx["ended_at"] = ended_at
prom = tx.get("promotion", {})
prom["status"] = "promote_failed"
prom["pre_op_id"] = pre_op_id
prom["error"] = error_msg
tx["promotion"] = prom
tx_file.write_text(json.dumps(tx, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
PY
}

extract_promotion_files() {
    local plan_json="$1"
    local pins_file="$2"
    local direct_out="$3"
    local override_out="$4"
    require_python
    "${PYTHON_BIN}" - "$plan_json" "$pins_file" "$direct_out" "$override_out" <<'PY'
import json
import pathlib
import sys

plan_path = pathlib.Path(sys.argv[1])
pins_path = pathlib.Path(sys.argv[2])
direct_out = pathlib.Path(sys.argv[3])
override_out = pathlib.Path(sys.argv[4])

plan = json.loads(plan_path.read_text(encoding="utf-8"))
direct = [x.strip() for x in plan.get("direct_additions", []) if str(x).strip()]
overrides = [x.strip() for x in plan.get("override_additions", []) if str(x).strip()]

pins = []
if pins_path.exists():
    pins = [line.strip() for line in pins_path.read_text(encoding="utf-8").splitlines() if line.strip()]

seen = set()
merged_override = []
for item in overrides + pins:
    if item not in seen:
        seen.add(item)
        merged_override.append(item)

direct_out.write_text("\n".join(sorted(set(direct))) + ("\n" if direct else ""), encoding="utf-8")
override_out.write_text("\n".join(merged_override) + ("\n" if merged_override else ""), encoding="utf-8")
PY
}

generate_promotion_plan() {
    local tx_file="$1"
    local plugin_path="$2"
    local out_json="$3"
    require_python
    "${PYTHON_BIN}" - "$tx_file" "$plugin_path" "$out_json" <<'PY'
import json
import pathlib
import re
import sys

tx_file = pathlib.Path(sys.argv[1])
plugin_path = pathlib.Path(sys.argv[2])
out_json = pathlib.Path(sys.argv[3])

tx = json.loads(tx_file.read_text(encoding="utf-8"))
added = tx.get("diff", {}).get("added", [])

def norm_name(spec: str) -> str:
    token = re.split(r"[<>=!~;\[]", spec, maxsplit=1)[0].strip()
    if "@" in token:
        token = token.split("@", 1)[0].strip()
    return re.sub(r"[-_.]+", "-", token).lower()

def parse_req_names(base: pathlib.Path):
    names = set()
    for f in sorted(base.glob("requirements*.txt")):
        for raw in f.read_text(encoding="utf-8", errors="ignore").splitlines():
            line = raw.strip()
            if not line or line.startswith("#"):
                continue
            if line.startswith(("-r", "--", "-c")):
                continue
            if line.startswith(("git+", "http://", "https://")):
                continue
            n = norm_name(line)
            if n:
                names.add(n)
    return names

req_names = parse_req_names(plugin_path) if plugin_path.exists() else set()

name_to_spec = {}
for spec in added:
    spec = str(spec).strip()
    if not spec:
        continue
    n = norm_name(spec)
    if n and n not in name_to_spec:
        name_to_spec[n] = spec

direct = []
overrides = []
for name, spec in sorted(name_to_spec.items()):
    if name in req_names:
        direct.append(spec)
    else:
        overrides.append(spec)

payload = {
    "direct_additions": direct,
    "override_additions": overrides,
}
out_json.write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
PY
}
write_conflict_report() {
    local txid="$1"
    local node_id="$2"
    local lock_log="$3"
    local out_json="${CONFLICT_DIR}/${txid}.json"
    require_python
    "${PYTHON_BIN}" - "$out_json" "$txid" "$node_id" "$lock_log" "$(timestamp_utc)" <<'PY'
import json
import pathlib
import re
import sys

out_path = pathlib.Path(sys.argv[1])
txid = sys.argv[2]
node_id = sys.argv[3]
lock_log = pathlib.Path(sys.argv[4])
created_at = sys.argv[5]

raw = ""
if lock_log.exists():
    raw = lock_log.read_text(encoding="utf-8", errors="ignore")

summary_lines = raw.splitlines()[:40]
packages = sorted(set(re.findall(r"([A-Za-z0-9_.-]+)==", raw)))

payload = {
    "txid": txid,
    "node_id": node_id,
    "created_at": created_at,
    "raw_log": str(lock_log),
    "summary": "\n".join(summary_lines),
    "detected_packages": packages,
    "input_hint": "Use: pkg==version",
}
out_path.write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
print(out_path)
PY
}

cmd_op_list() {
    ensure_layout
    require_python
    "${PYTHON_BIN}" - "$OPS_DIR" <<'PY'
import json
import pathlib
import sys

ops_dir = pathlib.Path(sys.argv[1])
items = []
for p in sorted([x for x in ops_dir.iterdir() if x.is_dir()], key=lambda x: x.name, reverse=True):
    meta = p / "meta.json"
    if not meta.exists():
        continue
    try:
        data = json.loads(meta.read_text(encoding="utf-8"))
    except Exception:
        continue
    items.append(data)

if not items:
    print("(no operations)")
else:
    for item in items:
        print(
            f"{item.get('op_id','')}  status={item.get('status','')}  kind={item.get('kind','')}  "
            f"ref={item.get('ref','')}  undoable={str(item.get('undoable', False)).lower()}  "
            f"started_at={item.get('started_at','')}"
        )
PY
}

cmd_op_inspect() {
    ensure_layout
    local op_id="${1:-}"
    if [ -z "$op_id" ]; then
        echo "Usage: gov op inspect <op_id>" >&2
        exit 1
    fi
    local meta_file="${OPS_DIR}/${op_id}/meta.json"
    if [ ! -f "$meta_file" ]; then
        echo "ERROR: operation not found: $op_id" >&2
        exit 2
    fi
    cat "$meta_file"
}

cmd_undo() {
    require_cmd uv
    ensure_layout
    require_python

    local op_id="${1:-}"
    if [ -z "$op_id" ]; then
        echo "Usage: gov undo <op_id>" >&2
        exit 1
    fi

    local meta_file
    meta_file="${OPS_DIR}/${op_id}/meta.json"
    if [ ! -f "$meta_file" ]; then
        echo "ERROR: operation not found: $op_id" >&2
        exit 2
    fi

    if ! "${PYTHON_BIN}" - "$meta_file" "$(file_sha256 "$PROJECT_FILE")" "$(file_sha256 "$LOCK_FILE")" "$(file_sha256 "$PLUGINS_FILE")" <<'PY'
import json
import pathlib
import sys

meta = pathlib.Path(sys.argv[1])
current = {
    "pyproject.toml": sys.argv[2],
    "uv.lock": sys.argv[3],
    "plugins.json": sys.argv[4],
}
data = json.loads(meta.read_text(encoding="utf-8"))
if data.get("status") != "success" or not data.get("undoable", False):
    print("ERROR: target operation is not undoable", file=sys.stderr)
    raise SystemExit(10)

files = data.get("files", {})
for key, now_hash in current.items():
    want = ((files.get(key) or {}).get("post_sha256") or "")
    if want != now_hash:
        print(f"ERROR: current state diverged for {key}; expected {want}, got {now_hash}", file=sys.stderr)
        raise SystemExit(11)
PY
    then
        exit 1
    fi

    local undo_op_id prod_env
    undo_op_id="$(op_begin "manual" "undo:${op_id}")"

    if ! op_restore_backup "$op_id"; then
        op_restore_backup "$undo_op_id" || true
        op_finalize "$undo_op_id" "failed" "false" "undo failed: missing target backup"
        exit 1
    fi

    prod_env="$(prod_env_path)"
    if ! UV_PROJECT_ENVIRONMENT="$prod_env" uv sync --locked --exact --all-groups; then
        op_restore_backup "$undo_op_id" || true
        UV_PROJECT_ENVIRONMENT="$prod_env" uv sync --locked --exact --all-groups || true
        op_finalize "$undo_op_id" "failed" "false" "undo failed: prod sync failed"
        exit 1
    fi

    "${PYTHON_BIN}" - "$meta_file" "$undo_op_id" "$(timestamp_utc)" <<'PY'
import json
import pathlib
import sys

meta = pathlib.Path(sys.argv[1])
undo_op_id = sys.argv[2]
ended_at = sys.argv[3]
data = json.loads(meta.read_text(encoding="utf-8"))
data["status"] = "undone"
data["undoable"] = False
data["ended_at"] = ended_at
data["note"] = f"undone by {undo_op_id}"
meta.write_text(json.dumps(data, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
PY

    op_finalize "$undo_op_id" "success" "true" "undo succeeded for ${op_id}"
    ops_prune

    echo "Undo complete."
    echo "  target_op_id: $op_id"
    echo "  undo_op_id: $undo_op_id"
}

write_tx_file() {
    local tx_file="$1"
    local txid="$2"
    local node_id="$3"
    local status="$4"
    local started_at="$5"
    local ended_at="$6"
    local candidate_env="$7"
    local pre_file="$8"
    local post_file="$9"
    local stdout_log="${10}"
    local stderr_log="${11}"
    local run_exit_code="${12}"
    local core_csv="${13}"
    require_python

    "${PYTHON_BIN}" - "$tx_file" "$txid" "$node_id" "$status" "$started_at" "$ended_at" "$candidate_env" "$pre_file" "$post_file" "$stdout_log" "$stderr_log" "$run_exit_code" "$core_csv" <<'PY'
import json
import pathlib
import re
import sys

(
    tx_file,
    txid,
    node_id,
    status,
    started_at,
    ended_at,
    candidate_env,
    pre_file,
    post_file,
    stdout_log,
    stderr_log,
    run_exit_code,
    core_csv,
) = sys.argv[1:14]

def read_lines(path: str):
    p = pathlib.Path(path)
    if not p.exists():
        return []
    return [line.strip() for line in p.read_text(encoding="utf-8").splitlines() if line.strip()]

def normalize_name(spec: str):
    token = re.split(r"[<>=!~\[]", spec, maxsplit=1)[0].strip()
    return re.sub(r"[-_.]+", "-", token).lower()

pre = read_lines(pre_file)
post = read_lines(post_file)
added = sorted(set(post) - set(pre))
removed = sorted(set(pre) - set(post))
core_set = {x.strip().lower() for x in core_csv.split(",") if x.strip()}
core_impact = sorted({n for n in [normalize_name(x) for x in added + removed] if n in core_set})

payload = {
    "txid": txid,
    "node_id": node_id,
    "started_at": started_at,
    "ended_at": ended_at,
    "candidate_env": candidate_env,
    "status": status,
    "pre_freeze": pre,
    "post_freeze": post,
    "diff": {
        "added": added,
        "removed": removed,
    },
    "core_impact": core_impact,
    "logs": {
        "stdout": stdout_log,
        "stderr": stderr_log,
        "run_exit_code": int(run_exit_code),
    },
    "conflict_report": "",
    "resolution_pins": [],
    "promotion_plan": {
        "direct_additions": [],
        "override_additions": [],
    },
    "promotion": {
        "status": "",
        "reason": "",
        "op_id": "",
        "pre_op_id": "",
        "error": "",
    },
}

pathlib.Path(tx_file).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
PY
}

cmd_init() {
    require_cmd uv
    ensure_layout
    ensure_project_file_for_init

    local prod_env
    prod_env="$(prod_env_path)"

    cd "$ROOT_DIR"
    uv lock
    UV_PROJECT_ENVIRONMENT="$prod_env" uv sync --locked --exact --all-groups

    echo "Initialized."
    echo "  root: $ROOT_DIR"
    echo "  prod env: $prod_env"
    echo "  lock file: $LOCK_FILE"
}

cmd_node_add() {
    require_cmd git
    ensure_layout

    local git_url="${1:-}"
    shift || true
    if [ -z "$git_url" ]; then
        echo "Usage: gov node add <git_url> [--ref <sha/tag>] [--id <node_id>]" >&2
        exit 1
    fi

    local ref=""
    local node_id=""
    while [ $# -gt 0 ]; do
        case "$1" in
            --ref)
                ref="${2:-}"
                shift 2
                ;;
            --id)
                node_id="${2:-}"
                shift 2
                ;;
            *)
                echo "Unknown argument: $1" >&2
                exit 1
                ;;
        esac
    done

    if [ -z "$node_id" ]; then
        node_id="$(basename "$git_url")"
        node_id="${node_id%.git}"
    fi

    local comfyui_dir custom_nodes_dir target_dir
    comfyui_dir="$(comfyui_dir_path)"
    custom_nodes_dir="${comfyui_dir}/custom_nodes"
    target_dir="${custom_nodes_dir}/${node_id}"

    mkdir -p "$custom_nodes_dir"
    if [ -e "$target_dir" ]; then
        echo "ERROR: node target already exists: $target_dir" >&2
        exit 1
    fi

    git clone "$git_url" "$target_dir"
    if [ -n "$ref" ]; then
        git -C "$target_dir" checkout "$ref"
    fi

    require_python
    "${PYTHON_BIN}" - "$PLUGINS_FILE" "$node_id" "$git_url" "$ref" "custom_nodes/${node_id}" "$(timestamp_utc)" <<'PY'
import json
import pathlib
import re
import sys

plugins_file = pathlib.Path(sys.argv[1])
node_id = sys.argv[2]
git_url = sys.argv[3]
ref = sys.argv[4]
install_relpath = sys.argv[5]
ts_now = sys.argv[6]

try:
    data = json.loads(plugins_file.read_text(encoding="utf-8")) if plugins_file.exists() else []
except Exception:
    data = []

if not isinstance(data, list):
    data = []

data = [x for x in data if not (isinstance(x, dict) and x.get("id") == node_id)]
data.append({
    "id": node_id,
    "git_url": git_url,
    "ref": ref,
    "install_relpath": install_relpath,
    "group": f"node-{re.sub(r'[-_.]+', '-', node_id.lower()).strip('-')}",
    "enabled": True,
    "managed_deps": [],
    "created_at": ts_now,
    "updated_at": ts_now,
})

plugins_file.write_text(json.dumps(data, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
PY

    echo "Node added."
    echo "  id: $node_id"
    echo "  path: $target_dir"
}
cmd_node_remove() {
    require_cmd uv
    ensure_layout

    local node_id="${1:-}"
    shift || true
    if [ -z "$node_id" ]; then
        echo "Usage: gov node remove <node_id> [--purge-code]" >&2
        exit 1
    fi

    local purge_code=false
    while [ $# -gt 0 ]; do
        case "$1" in
            --purge-code)
                purge_code=true
                shift
                ;;
            *)
                echo "Unknown argument: $1" >&2
                exit 1
                ;;
        esac
    done

    local meta_json deps_file
    meta_json="$(mktemp)"
    deps_file="$(mktemp)"
    if ! plugin_get_meta "$node_id" "$meta_json"; then
        rm -f "$meta_json" "$deps_file"
        echo "ERROR: node not found: $node_id" >&2
        exit 2
    fi

    local group plugin_relpath plugin_path
    group="$("${PYTHON_BIN}" - "$meta_json" <<'PY'
import json
import pathlib
import sys
meta = json.loads(pathlib.Path(sys.argv[1]).read_text(encoding="utf-8"))
print(meta.get("group") or f"node-{meta.get('id')}")
PY
)"
    group="$(normalize_group_name "$group")"
    if [ -z "$group" ]; then
        group="$(group_name_for_node "$node_id")"
    fi
    plugin_relpath="$("${PYTHON_BIN}" - "$meta_json" <<'PY'
import json
import pathlib
import sys
meta = json.loads(pathlib.Path(sys.argv[1]).read_text(encoding="utf-8"))
print(meta.get("install_relpath") or f"custom_nodes/{meta.get('id', '')}")
PY
)"
    plugin_path="$(plugin_install_abs_path "$plugin_relpath")"

    local op_id
    op_id="$(op_begin "remove" "$node_id")"

    local workdir lock_log
    workdir="$(build_workdir_for_tx "remove-${node_id}")"
    lock_log="${LOG_DIR}/remove-${node_id}.lock.log"
    write_group_deps "$workdir/pyproject.toml" "$group" "$deps_file"

    (
        cd "$workdir"
        if [ -s "$deps_file" ]; then
            while IFS= read -r dep; do
                [ -z "$dep" ] && continue
                uv remove --group "$group" "$dep" || true
            done < "$deps_file"
        fi
        uv lock
    ) >"$lock_log" 2>&1 || {
        op_restore_backup "$op_id" || true
        op_finalize "$op_id" "failed" "false" "node remove lock failed"
        rm -f "$meta_json" "$deps_file"
        echo "ERROR: node remove lock failed. See $lock_log" >&2
        exit 1
    }

    cp "$workdir/pyproject.toml" "$PROJECT_FILE"
    cp "$workdir/uv.lock" "$LOCK_FILE"

    local prod_env
    prod_env="$(prod_env_path)"
    if ! UV_PROJECT_ENVIRONMENT="$prod_env" uv sync --locked --exact --all-groups; then
        echo "ERROR: prod sync failed during node remove, restoring pre-operation state: $op_id" >&2
        op_restore_backup "$op_id"
        UV_PROJECT_ENVIRONMENT="$prod_env" uv sync --locked --exact --all-groups || true
        op_finalize "$op_id" "failed" "false" "prod sync failed"
        rm -f "$meta_json" "$deps_file"
        exit 1
    fi

    plugin_remove_record "$node_id"

    if [ "$purge_code" = true ] && [ -n "$plugin_path" ] && [ -d "$plugin_path" ]; then
        rm -rf "$plugin_path"
    fi

    op_finalize "$op_id" "success" "true" "node remove succeeded; purge_code=${purge_code}"
    ops_prune

    rm -f "$meta_json" "$deps_file"

    echo "Node removed."
    echo "  id: $node_id"
    echo "  op_id: $op_id"
    if [ "$purge_code" = true ]; then
        echo "  note: code purge is irreversible; undo will not restore plugin files"
    fi
}

cmd_tx_run() {
    require_cmd uv
    ensure_layout

    local node_id="${1:-}"
    shift || true
    if [ -z "$node_id" ]; then
        echo "Usage: gov tx run <node_id> [--timeout <seconds>]" >&2
        exit 1
    fi

    local timeout_sec
    timeout_sec="$(tx_timeout_seconds)"
    while [ $# -gt 0 ]; do
        case "$1" in
            --timeout)
                timeout_sec="${2:-}"
                shift 2
                ;;
            *)
                echo "Unknown argument: $1" >&2
                exit 1
                ;;
        esac
    done

    if ! ensure_plugin_exists "$node_id"; then
        echo "ERROR: node not found in plugins registry: $node_id" >&2
        exit 1
    fi

    local txid candidate_root candidate_env pre_file post_file stdout_log stderr_log started_at ended_at run_exit_code status
    txid="$(new_txid)"
    candidate_root="$(candidate_root_path)"
    candidate_env="${candidate_root}/${txid}"
    TX_FILE="${TX_DIR}/${txid}.json"
    pre_file="${TX_DIR}/${txid}.pre.freeze.txt"
    post_file="${TX_DIR}/${txid}.post.freeze.txt"
    stdout_log="${LOG_DIR}/${txid}.stdout.log"
    stderr_log="${LOG_DIR}/${txid}.stderr.log"

    started_at="$(timestamp_utc)"
    run_exit_code=0

    mkdir -p "$candidate_root"
    : > "$pre_file"
    : > "$post_file"
    : > "$stdout_log"
    : > "$stderr_log"

    write_tx_file "$TX_FILE" "$txid" "$node_id" "running" "$started_at" "" "$candidate_env" "$pre_file" "$post_file" "$stdout_log" "$stderr_log" "-1" "$(core_packages_csv)"

    cd "$ROOT_DIR"
    if [ ! -f "$LOCK_FILE" ]; then
        uv lock
    fi
    UV_PROJECT_ENVIRONMENT="$candidate_env" uv sync --locked --exact --all-groups

    collect_freeze_file "$candidate_env" "$pre_file"

    local comfyui_dir main_py
    comfyui_dir="$(comfyui_dir_path)"
    main_py="${comfyui_dir}/main.py"

    set +e
    if [ -f "$main_py" ]; then
        if command -v timeout >/dev/null 2>&1; then
            timeout "${timeout_sec}s" "$candidate_env/bin/python" "$main_py" >"$stdout_log" 2>"$stderr_log"
            run_exit_code=$?
        else
            "$candidate_env/bin/python" "$main_py" >"$stdout_log" 2>"$stderr_log"
            run_exit_code=$?
        fi
    else
        echo "ComfyUI entry not found: $main_py" >"$stderr_log"
        run_exit_code=2
    fi
    set -e

    collect_freeze_file "$candidate_env" "$post_file"

    ended_at="$(timestamp_utc)"
    status="completed"
    if [ "$run_exit_code" -ne 0 ]; then
        status="failed"
    fi

    write_tx_file "$TX_FILE" "$txid" "$node_id" "$status" "$started_at" "$ended_at" "$candidate_env" "$pre_file" "$post_file" "$stdout_log" "$stderr_log" "$run_exit_code" "$(core_packages_csv)"

    echo "Transaction recorded."
    echo "  txid: $txid"
    echo "  status: $status"
    echo "  file: $TX_FILE"
}

cmd_tx_inspect() {
    local txid="${1:-}"
    if [ -z "$txid" ]; then
        echo "Usage: gov tx inspect <txid>" >&2
        exit 1
    fi
    load_tx "$txid"

    require_python
    "${PYTHON_BIN}" - "$TX_FILE" <<'PY'
import json
import pathlib
import sys

tx = json.loads(pathlib.Path(sys.argv[1]).read_text(encoding="utf-8"))
print(f"txid: {tx.get('txid')}")
print(f"node_id: {tx.get('node_id')}")
print(f"status: {tx.get('status')}")
print(f"started_at: {tx.get('started_at')}")
print(f"ended_at: {tx.get('ended_at')}")
print(f"candidate_env: {tx.get('candidate_env')}")
diff = tx.get("diff", {})
print(f"added_count: {len(diff.get('added', []))}")
print(f"removed_count: {len(diff.get('removed', []))}")
core = tx.get("core_impact", [])
print(f"core_impact: {', '.join(core) if core else '(none)'}")
print(f"conflict_report: {tx.get('conflict_report') or '(none)'}")
print(f"resolution_pins: {len(tx.get('resolution_pins', []))}")
prom = tx.get("promotion", {})
print(f"promotion_status: {prom.get('status') or '(none)'}")
logs = tx.get("logs", {})
print(f"stdout_log: {logs.get('stdout', '')}")
print(f"stderr_log: {logs.get('stderr', '')}")
print(f"run_exit_code: {logs.get('run_exit_code', '')}")
PY
}

cmd_tx_abort() {
    local txid="${1:-}"
    if [ -z "$txid" ]; then
        echo "Usage: gov tx abort <txid>" >&2
        exit 1
    fi
    load_tx "$txid"

    local candidate_env
    candidate_env="$(tx_get_field "$TX_FILE" "candidate_env" "")"
    if [ -n "$candidate_env" ] && [ -d "$candidate_env" ]; then
        rm -rf "$candidate_env"
    fi

    tx_update_status "$TX_FILE" "aborted"
    echo "Transaction aborted."
    echo "  txid: $txid"
}
cmd_resolve() {
    require_cmd uv
    require_python
    ensure_layout

    local txid="${1:-}"
    if [ -z "$txid" ]; then
        echo "Usage: gov resolve <txid>" >&2
        exit 1
    fi

    load_tx "$txid"

    local status node_id conflict_report
    status="$(tx_get_field "$TX_FILE" "status" "")"
    node_id="$(tx_get_field "$TX_FILE" "node_id" "")"
    conflict_report="$(tx_get_field "$TX_FILE" "conflict_report" "")"

    case "$status" in
        needs_resolution|promote_failed|resolved)
            ;;
        *)
            echo "ERROR: tx status '$status' is not resolvable" >&2
            exit 1
            ;;
    esac

    if [ -n "$conflict_report" ] && [ -f "$conflict_report" ]; then
        echo "=== Conflict Summary ==="
        "${PYTHON_BIN}" - "$conflict_report" <<'PY'
import json
import pathlib
import sys
r = json.loads(pathlib.Path(sys.argv[1]).read_text(encoding="utf-8"))
print(r.get("summary", "(no summary)"))
PY
        echo "========================"
    fi

    local new_pins merged_pins
    new_pins="$(mktemp)"
    merged_pins="$(mktemp)"

    echo "Input pins as pkg==version (empty line to finish):"
    while true; do
        local line
        IFS= read -r line || true
        [ -z "$line" ] && break
        if [[ ! "$line" =~ ^[A-Za-z0-9_.-]+==[^[:space:]]+$ ]]; then
            echo "Invalid pin format: $line" >&2
            continue
        fi
        echo "$line" >> "$new_pins"
    done

    require_python
    "${PYTHON_BIN}" - "$TX_FILE" "$new_pins" "$merged_pins" <<'PY'
import json
import pathlib
import sys

tx = json.loads(pathlib.Path(sys.argv[1]).read_text(encoding="utf-8"))
new_pins = pathlib.Path(sys.argv[2])
out = pathlib.Path(sys.argv[3])

extra = []
if new_pins.exists():
    extra = [x.strip() for x in new_pins.read_text(encoding="utf-8").splitlines() if x.strip()]

seen = set()
merged = []
for item in tx.get("resolution_pins", []) + extra:
    if item not in seen:
        seen.add(item)
        merged.append(item)

out.write_text("\n".join(merged) + ("\n" if merged else ""), encoding="utf-8")
PY

    local meta_json group plugin_relpath plugin_path plan_json workdir direct_file override_file lock_log report
    meta_json="$(mktemp)"
    if ! plugin_get_meta "$node_id" "$meta_json"; then
        rm -f "$meta_json" "$new_pins" "$merged_pins"
        echo "ERROR: plugin metadata missing for node_id=$node_id" >&2
        exit 1
    fi

    plugin_relpath="$("${PYTHON_BIN}" - "$meta_json" <<'PY'
import json
import pathlib
import sys
m = json.loads(pathlib.Path(sys.argv[1]).read_text(encoding="utf-8"))
print(m.get("install_relpath") or f"custom_nodes/{m.get('id', '')}")
PY
)"
    plugin_path="$(plugin_install_abs_path "$plugin_relpath")"
    group="$("${PYTHON_BIN}" - "$meta_json" <<'PY'
import json
import pathlib
import sys
m = json.loads(pathlib.Path(sys.argv[1]).read_text(encoding="utf-8"))
print(m.get("group") or f"node-{m.get('id', '')}")
PY
)"
    group="$(normalize_group_name "$group")"
    if [ -z "$group" ]; then
        group="$(group_name_for_node "$node_id")"
    fi

    plan_json="$(mktemp)"
    generate_promotion_plan "$TX_FILE" "$plugin_path" "$plan_json"
    tx_set_plan "$TX_FILE" "$plan_json"

    workdir="$(build_workdir_for_tx "$txid")"
    direct_file="${WORK_DIR}/${txid}.resolve.direct.txt"
    override_file="${WORK_DIR}/${txid}.resolve.override.txt"
    extract_promotion_files "$plan_json" "$merged_pins" "$direct_file" "$override_file"

    lock_log="${CONFLICT_DIR}/${txid}.resolve.lock.log"
    if apply_plan_in_workdir "$workdir" "$group" "$direct_file" "$override_file" "$lock_log"; then
        tx_set_resolution_pins "$TX_FILE" "$merged_pins" "resolved"
        echo "Resolve succeeded."
        echo "  txid: $txid"
        echo "  status: resolved"
    else
        report="$(write_conflict_report "$txid" "$node_id" "$lock_log")"
        tx_set_resolution_pins "$TX_FILE" "$merged_pins" "needs_resolution"
        tx_set_conflict "$TX_FILE" "$report" "needs_resolution" ""
        echo "Resolve failed."
        echo "  txid: $txid"
        echo "  conflict_report: $report"
        rm -f "$meta_json" "$new_pins" "$merged_pins" "$plan_json"
        exit 2
    fi

    rm -f "$meta_json" "$new_pins" "$merged_pins" "$plan_json"
}

cmd_tx_promote() {
    require_cmd uv
    require_python
    ensure_layout

    local txid="${1:-}"
    shift || true
    if [ -z "$txid" ]; then
        echo "Usage: gov tx promote <txid> [--approve-core --reason \"...\"] [--allow-failed-run]" >&2
        exit 1
    fi

    local approve_core=false
    local reason=""
    local allow_failed_run=false
    while [ $# -gt 0 ]; do
        case "$1" in
            --approve-core)
                approve_core=true
                shift
                ;;
            --reason)
                reason="${2:-}"
                shift 2
                ;;
            --allow-failed-run)
                allow_failed_run=true
                shift
                ;;
            *)
                echo "Unknown argument: $1" >&2
                exit 1
                ;;
        esac
    done

    load_tx "$txid"

    local status node_id core_count
    status="$(tx_get_field "$TX_FILE" "status" "")"
    node_id="$(tx_get_field "$TX_FILE" "node_id" "")"
    core_count="$(tx_get_field "$TX_FILE" "core_impact" "" | sed '/^$/d' | wc -l | tr -d ' ')"

    case "$status" in
        completed|resolved)
            ;;
        failed)
            if [ "$allow_failed_run" != true ]; then
                echo "ERROR: tx status is failed. Use --allow-failed-run to continue." >&2
                exit 1
            fi
            ;;
        *)
            echo "ERROR: tx status '$status' cannot be promoted" >&2
            exit 1
            ;;
    esac

    if [ "$core_count" != "0" ] && [ "$approve_core" != true ]; then
        echo "ERROR: core package impact detected. Use --approve-core to continue." >&2
        exit 1
    fi

    local meta_json group plugin_relpath plugin_path op_id workdir plan_json pins_file direct_file override_file lock_log conflict_report
    meta_json="$(mktemp)"
    if ! plugin_get_meta "$node_id" "$meta_json"; then
        rm -f "$meta_json"
        echo "ERROR: plugin metadata missing for node=$node_id" >&2
        exit 1
    fi

    plugin_relpath="$("${PYTHON_BIN}" - "$meta_json" <<'PY'
import json
import pathlib
import sys
m = json.loads(pathlib.Path(sys.argv[1]).read_text(encoding="utf-8"))
print(m.get("install_relpath") or f"custom_nodes/{m.get('id', '')}")
PY
)"
    plugin_path="$(plugin_install_abs_path "$plugin_relpath")"
    group="$("${PYTHON_BIN}" - "$meta_json" <<'PY'
import json
import pathlib
import sys
m = json.loads(pathlib.Path(sys.argv[1]).read_text(encoding="utf-8"))
print(m.get("group") or f"node-{m.get('id', '')}")
PY
)"
    group="$(normalize_group_name "$group")"
    if [ -z "$group" ]; then
        group="$(group_name_for_node "$node_id")"
    fi

    op_id="$(op_begin "promote" "$txid")"

    workdir="$(build_workdir_for_tx "$txid")"
    plan_json="$(mktemp)"
    generate_promotion_plan "$TX_FILE" "$plugin_path" "$plan_json"
    tx_set_plan "$TX_FILE" "$plan_json"

    pins_file="$(mktemp)"
    tx_get_field "$TX_FILE" "resolution_pins" "" > "$pins_file" || true

    direct_file="${WORK_DIR}/${txid}.promote.direct.txt"
    override_file="${WORK_DIR}/${txid}.promote.override.txt"
    extract_promotion_files "$plan_json" "$pins_file" "$direct_file" "$override_file"

    lock_log="${CONFLICT_DIR}/${txid}.promote.lock.log"
    if ! apply_plan_in_workdir "$workdir" "$group" "$direct_file" "$override_file" "$lock_log"; then
        conflict_report="$(write_conflict_report "$txid" "$node_id" "$lock_log")"
        tx_set_conflict "$TX_FILE" "$conflict_report" "needs_resolution" "$op_id"
        op_restore_backup "$op_id" || true
        op_finalize "$op_id" "failed" "false" "promote lock conflict"
        echo "Promote blocked by lock conflict."
        echo "  txid: $txid"
        echo "  conflict_report: $conflict_report"
        rm -f "$meta_json" "$plan_json" "$pins_file"
        exit 2
    fi

    cp "$workdir/pyproject.toml" "$PROJECT_FILE"
    cp "$workdir/uv.lock" "$LOCK_FILE"

    local prod_env
    prod_env="$(prod_env_path)"
    if ! UV_PROJECT_ENVIRONMENT="$prod_env" uv sync --locked --exact --all-groups; then
        echo "ERROR: prod sync failed, restoring pre-operation state: $op_id" >&2
        op_restore_backup "$op_id"
        UV_PROJECT_ENVIRONMENT="$prod_env" uv sync --locked --exact --all-groups || true
        tx_set_promote_failed "$TX_FILE" "$op_id" "prod sync failed"
        op_finalize "$op_id" "failed" "false" "prod sync failed"
        rm -f "$meta_json" "$plan_json" "$pins_file"
        exit 1
    fi

    local smoke_cmd
    smoke_cmd="$(smoke_test_cmd)"
    if [ -n "$smoke_cmd" ]; then
        local comfyui_dir
        comfyui_dir="$(comfyui_dir_path)"
        if ! (cd "$comfyui_dir" && PATH="$prod_env/bin:$PATH" bash -lc "$smoke_cmd"); then
            echo "ERROR: smoke test failed, restoring pre-operation state: $op_id" >&2
            op_restore_backup "$op_id"
            UV_PROJECT_ENVIRONMENT="$prod_env" uv sync --locked --exact --all-groups || true
            tx_set_promote_failed "$TX_FILE" "$op_id" "smoke test failed"
            op_finalize "$op_id" "failed" "false" "smoke test failed"
            rm -f "$meta_json" "$plan_json" "$pins_file"
            exit 1
        fi
    else
        if ! "$prod_env/bin/python" -c "import sys; print(sys.version)" >/dev/null 2>&1; then
            echo "ERROR: default smoke test failed, restoring pre-operation state: $op_id" >&2
            op_restore_backup "$op_id"
            UV_PROJECT_ENVIRONMENT="$prod_env" uv sync --locked --exact --all-groups || true
            tx_set_promote_failed "$TX_FILE" "$op_id" "default smoke test failed"
            op_finalize "$op_id" "failed" "false" "default smoke test failed"
            rm -f "$meta_json" "$plan_json" "$pins_file"
            exit 1
        fi
    fi

    tx_set_promoted "$TX_FILE" "$op_id" "$reason"
    plugin_update_after_promote "$node_id" "$PROJECT_FILE"
    op_finalize "$op_id" "success" "true" "promote succeeded"
    ops_prune

    echo "Promote successful."
    echo "  txid: $txid"
    echo "  op_id: $op_id"

    rm -f "$meta_json" "$plan_json" "$pins_file"
}
cmd_status() {
    ensure_layout
    local prod_env
    prod_env="$(prod_env_path)"
    require_python
    "${PYTHON_BIN}" - "$prod_env" "$LOCK_FILE" "$TX_DIR" <<'PY'
import json
import pathlib
import sys

prod_env = pathlib.Path(sys.argv[1])
lock_file = pathlib.Path(sys.argv[2])
tx_dir = pathlib.Path(sys.argv[3])

records = []
for f in sorted(tx_dir.glob("*.json")):
    try:
        records.append(json.loads(f.read_text(encoding="utf-8")))
    except Exception:
        continue

pending_status = {
    "running",
    "completed",
    "failed",
    "needs_resolution",
    "resolved",
    "promote_failed",
}
pending = [x for x in records if x.get("status") in pending_status]
recent = sorted(records, key=lambda x: x.get("started_at", ""), reverse=True)[:8]

print("Comfy Env Status")
print(f"prod_env_exists: {'yes' if prod_env.exists() else 'no'}")
print(f"lock_exists: {'yes' if lock_file.exists() else 'no'}")
print(f"transactions_total: {len(records)}")
print(f"transactions_pending: {len(pending)}")
print("recent_transactions:")
if not recent:
    print("  (none)")
else:
    for item in recent:
        print(f"  - {item.get('txid')} [{item.get('status')}] node={item.get('node_id')}")
PY
}

cmd_run() {
    ensure_layout

    local sync_before=false
    local comfyui_args=()

    while [ $# -gt 0 ]; do
        case "$1" in
            --sync)
                sync_before=true
                shift
                ;;
            --)
                shift
                comfyui_args=("$@")
                break
                ;;
            *)
                comfyui_args+=("$1")
                shift
                ;;
        esac
    done

    local cfg_sync
    cfg_sync="$(config_get "run.sync_before_run" "false")"
    if [ "$cfg_sync" = "true" ]; then
        sync_before=true
    fi

    local prod_env comfyui_dir main_py
    prod_env="$(prod_env_path)"
    comfyui_dir="$(comfyui_dir_path)"
    main_py="${comfyui_dir}/main.py"

    if [ ! -d "$prod_env" ]; then
        echo "ERROR: prod environment not found: $prod_env" >&2
        echo "Hint: run 'gov init' to create the production environment." >&2
        exit 1
    fi

    if [ ! -f "$LOCK_FILE" ]; then
        echo "ERROR: uv.lock not found. Run 'gov init' first." >&2
        exit 1
    fi

    if [ ! -f "$main_py" ]; then
        echo "ERROR: ComfyUI entry not found: $main_py" >&2
        exit 1
    fi

    if [ "$sync_before" = true ]; then
        require_cmd uv
        echo "Syncing prod environment..."
        UV_PROJECT_ENVIRONMENT="$prod_env" uv sync --locked --exact --all-groups
    fi

    local extra_args
    extra_args="$(config_get "run.extra_args" "")"

    echo "$$" > "$PID_FILE"

    echo "Starting ComfyUI with prod environment..."
    echo "  python: $prod_env/bin/python"
    echo "  entry:  $main_py"
    echo "  pid:    $$"

    if [ ${#comfyui_args[@]} -gt 0 ]; then
        exec "$prod_env/bin/python" "$main_py" "${comfyui_args[@]}"
    elif [ -n "$extra_args" ]; then
        # Word splitting on purpose to pass args
        # shellcheck disable=SC2086
        exec "$prod_env/bin/python" "$main_py" $extra_args
    else
        exec "$prod_env/bin/python" "$main_py"
    fi
}

cmd_stop() {
    if [ ! -f "$PID_FILE" ]; then
        echo "ERROR: PID file not found: $PID_FILE" >&2
        echo "ComfyUI might not be running via 'gov run'." >&2
        exit 1
    fi

    local pid
    pid="$(cat "$PID_FILE")"
    
    if ! kill -0 "$pid" 2>/dev/null; then
        echo "Process $pid is not running. Cleaning up stale PID file."
        rm -f "$PID_FILE"
        exit 0
    fi

    echo "Stopping ComfyUI (PID: $pid)..."
    kill -TERM "$pid"

    local wait_time=0
    local max_wait=30
    while kill -0 "$pid" 2>/dev/null; do
        sleep 1
        wait_time=$((wait_time + 1))
        if [ "$wait_time" -ge "$max_wait" ]; then
            echo "Timeout reached. Force killing process $pid..."
            kill -KILL "$pid" 2>/dev/null || true
            break
        fi
    done

    rm -f "$PID_FILE"
    echo "ComfyUI stopped."
}

cmd_help() {
    cat <<'EOF'
Comfy Env Governance CLI (Batch-3)

Usage:
  gov init
  gov node add <git_url> [--ref <sha/tag>] [--id <node_id>]
  gov node remove <node_id> [--purge-code]
  gov tx run <node_id> [--timeout <seconds>]
  gov tx inspect <txid>
  gov tx abort <txid>
  gov tx promote <txid> [--approve-core --reason "..."] [--allow-failed-run]
  gov resolve <txid>
  gov undo <op_id>
  gov op list
  gov op inspect <op_id>
  gov status
  gov run [--sync] [-- <args...>]
  gov stop

Notes:
  - Operation backups are stored in state/ops/<op_id>/backup.
  - Undo requires current file hashes to match target operation post hashes.
  - Ops retention defaults to 100.
  - Resolve pins are persisted in dependency-groups.overrides.
EOF
}

main() {
    local cmd="${1:-}"
    case "$cmd" in
        init)
            shift
            cmd_init "$@"
            ;;
        node)
            shift
            case "${1:-}" in
                add)
                    shift
                    cmd_node_add "$@"
                    ;;
                remove)
                    shift
                    cmd_node_remove "$@"
                    ;;
                *)
                    echo "Usage: gov node {add|remove} ..." >&2
                    exit 1
                    ;;
            esac
            ;;
        tx)
            shift
            case "${1:-}" in
                run)
                    shift
                    cmd_tx_run "$@"
                    ;;
                inspect)
                    shift
                    cmd_tx_inspect "$@"
                    ;;
                abort)
                    shift
                    cmd_tx_abort "$@"
                    ;;
                promote)
                    shift
                    cmd_tx_promote "$@"
                    ;;
                *)
                    echo "Usage: gov tx {run|inspect|abort|promote} ..." >&2
                    exit 1
                    ;;
            esac
            ;;
        resolve)
            shift
            cmd_resolve "$@"
            ;;
        undo)
            shift
            cmd_undo "$@"
            ;;
        op)
            shift
            case "${1:-}" in
                list)
                    shift
                    cmd_op_list "$@"
                    ;;
                inspect)
                    shift
                    cmd_op_inspect "$@"
                    ;;
                *)
                    echo "Usage: gov op {list|inspect} ..." >&2
                    exit 1
                    ;;
            esac
            ;;
        status)
            shift
            cmd_status "$@"
            ;;
        run)
            shift
            cmd_run "$@"
            ;;
        stop)
            shift
            cmd_stop "$@"
            ;;
        ""|-h|--help|help)
            cmd_help
            ;;
        *)
            echo "Unknown command: $cmd" >&2
            cmd_help
            exit 1
            ;;
    esac
}

main "$@"
